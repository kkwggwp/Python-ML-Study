[7/3 오전 복습]
KNN(하이퍼 파라미터 정리)

metrics(거리 측정 방식)
민코프스키 거리공식 : 맨해튼 거리공식과 유클리디언 거리공식을 일반화 시켜서 하나로 통합
- 맨해튼 거리공식
- 유클리디언 거리공식
> 거리공식을 선택하는 법 : 하이퍼 파라미터 내부에 p값을 1 / 2설정
   1로 설정한 경우 : 맨해튼 거리공식
   2로 설정한 경우 : 유클리디언 거리공식

n_neighbors : 새로운 데이터 포인트를 분류할때 살펴볼 이웃의 수
> 이웃 데이터 몇개 볼건가?

weight : 가중치 결정 방법
- uniform : 데이터에 부여되는 가중치가 모두 동일하게 부여
- distance : 거리에 반비례 가중치 부여
> 거리가 가까울수록 가중치 증가 / 거리가 멀수록 가중치 감소

의사결정 나무(Decision Tree)
- 질문을 던지고 값을 True / False로 분류해서 데이터를 분할해 나가는 방식
- 나무가 가지 치면서 자라는 방식으로 트리가 작동하기 때문에 트리모델이라고 부른다.
- 실제 나무와 다른점이 있다면 이 트리 모델은 거꾸로 자라는 모델이다.

지니 불순도
- 데이터를 분할하기 위한 기준
- 질문을 던질때 지니 불순도가 빠르게 낮아지도록 데이터를 분할하는 방법을 선택
- 0 ~ 0.5
- 0에 가까워질수록 좋은 분할 기준(질문을 잘 선정했다.)
- 0.5에 가까워질수록 안좋은 분할 기준(질문이 안좋다.)
- 지니 불순도는 데이터가 얼마나 섞였는지 수치로 확인시켜주는 방법

엔트로피
- 데이터를 분할하기 위한 기준
- 질문을 던질때 엔트로피가 빠르게 낮아지도록 데이터를 분할하는 방법을 선택
- 0 ~ 1
- 엔트로피가 낮아질 수록 좋은 분할 기준(질문을 잘 선정했다.)
- 엔트로피가 높아질 수록 안좋은 분할 기준(질문이 안좋다.)
- 엔트로피는 데이터가 얼마나 순수한지 수치로 확인시켜주는 방법

Node
-> Root Node : 처음 모든 데이터가 모이는 트리에 시작점
-> Decision Node : 중간에 분할되면서 생성되는 노드(데이터 집합점) - 분할 가능
-> Leaf Node : 잎사귀 노드 / 최말단 노드 - 더 이상 분할될 수 없는 노드
> 노드가 분할될 때마다 Depth(깊이)가 증가 
> Depth가 1이 깊어질때마다 노드가 분할되었다!

사전 가지치기(하이퍼 파라미터 튜닝)
> 사전 가지치기를 진행하는 이유 : 과대적합을 제어하기 위해서
- max_depth : 최대 깊이 제어
  -> 값을 크게 줄 수록 과대적합 위험성 증가 / 값을 너무 작게 주면 모델이 단순해짐(과소적합)
- min_samples_split : 노드를 분할하기 위해 데이터를 얼마나 가지고 있어야 하는가?
  -> 값을 작게 줄수록 트리가 세분화 > 과대적합 / 값을 너무 크게 주면 과소적합
  -> 전체 데이터 8120 > min_samples_split = 8000 설정시 > 트리 1번 쪼개짐(과소적합)
- min_samples_leaf : 데이터를 분할한 노드가 가져야할 최소 샘플 수
  -> 쪼갰을때 최소 샘플수보다 작은 노드가 발생한다-> 노드를 안쪼갠다.
  -> 값이 작을수록 과대적합 / 값이 너무 크면 과소적합
- max_leaf_nodes : 트리의 잎사귀 노드 갯수
  -> 값이 클수록 과대 적합

plot_tree : 학습된 트리모델이 어떻게 생겼는지 시각화 시켜주는 함수
plot_tree (tree, # 학습된 트리모델
	     feature_names = ,# 트리를 분할할때 사용하는 특성의 이름
	     class_names = ,# 실제 정답 이름
	     font_size = ,# 글씨의 크기
	     filled = )# True / False : 트리 내부에 색상을 채워줄 것인가?
# 피봇테이블 - 코드 실습에서 확인